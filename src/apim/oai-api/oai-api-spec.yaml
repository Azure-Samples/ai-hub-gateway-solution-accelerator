openapi: 3.0.1
info:
  title: Azure OpenAI API
  description: Azure OpenAI APIs for completions and search
  version: '1.0'
servers:
  - url: https://REPLACE.COM/openai
paths:
  '/deployments/{deployment-id}/completions':
    post:
      summary: Completions
      description: 'Creates a completion for the provided prompt, parameters and chosen model.'
      operationId: Completions_Create
      parameters:
        - name: deployment-id
          in: path
          required: true
          schema:
            enum:
              - gpt-35-turbo
            type: string
            description: Deployment id of the model which was deployed.
            default: gpt-35-turbo
            example: davinci
        - name: api-version
          in: query
          required: true
          schema:
            enum:
              - '2023-05-15'
            type: string
            description: api version
            default: '2023-05-15'
            example: '2023-05-15'
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                prompt:
                  oneOf:
                    - type: string
                      default: ''
                      nullable: true
                      example: This is a test.
                    - type: array
                      items:
                        type: string
                        default: ''
                        example: This is a test.
                      description: Array size minimum of 1 and maximum of 2048
                  description: "The prompt(s) to generate completions for, encoded as a string or array of strings.\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. Maximum allowed size of string list is 2048."
                max_tokens:
                  type: integer
                  description: 'The token count of your prompt plus max_tokens cannot exceed the model''s context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0.'
                  default: 16
                  nullable: true
                  example: 16
                temperature:
                  type: number
                  description: "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.\nWe generally recommend altering this or top_p but not both."
                  default: 1
                  nullable: true
                  example: 1
                top_p:
                  type: number
                  description: "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both."
                  default: 1
                  nullable: true
                  example: 1
                logit_bias:
                  type: object
                  description: 'Defaults to null. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {"50256" &#58; -100} to prevent the <|endoftext|> token from being generated.'
                user:
                  type: string
                  description: 'A unique identifier representing your end-user, which can help monitoring and detecting abuse'
                n:
                  type: integer
                  description: "How many completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.\nNote: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop."
                  default: 1
                  nullable: true
                  example: 1
                stream:
                  type: boolean
                  description: 'Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.'
                  default: false
                  nullable: true
                logprobs:
                  type: integer
                  description: "Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.\nMinimum of 0 and maximum of 5 allowed."
                  default: 
                  nullable: true
                model:
                  type: string
                  description: 'ID of the model to use. You can use the Models_List operation to see all of your available models, or see our Models_Get overview for descriptions of them.'
                  nullable: true
                  example: davinci
                suffix:
                  type: string
                  description: The suffix that comes after a completion of inserted text.
                  nullable: true
                echo:
                  type: boolean
                  description: Echo back the prompt in addition to the completion
                  default: false
                  nullable: true
                stop:
                  oneOf:
                    - type: string
                      default: <|endoftext|>
                      nullable: true
                      example: "\n"
                    - type: array
                      items:
                        type: string
                        example:
                          - "\n"
                      description: Array minimum size of 1 and maximum of 4
                  description: Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
                completion_config:
                  type: string
                  nullable: true
                presence_penalty:
                  type: number
                  description: 'Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model''s likelihood to talk about new topics.'
                  default: 0
                frequency_penalty:
                  type: number
                  description: 'Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model''s likelihood to repeat the same line verbatim.'
                  default: 0
                best_of:
                  type: integer
                  description: "Generates best_of completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\nWhen used with n, best_of controls the number of candidate completions and n specifies how many to return â€“ best_of must be greater than n.\nNote: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. Has maximum value of 128."
            example:
              prompt: "Negate the following sentence.The price for bubblegum increased on thursday.\n\n Negated Sentence:"
              max_tokens: 50
      responses:
        '200':
          description: OK
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              schema:
                type: string
          content:
            application/json:
              schema:
                required:
                  - id
                  - object
                  - created
                  - model
                  - choices
                type: object
                properties:
                  id:
                    type: string
                  object:
                    type: string
                  created:
                    type: integer
                  model:
                    type: string
                  choices:
                    type: array
                    items:
                      type: object
                      properties:
                        text:
                          type: string
                        index:
                          type: integer
                        logprobs:
                          type: object
                          properties:
                            tokens:
                              type: array
                              items:
                                type: string
                            token_logprobs:
                              type: array
                              items:
                                type: number
                            top_logprobs:
                              type: array
                              items:
                                type: object
                                additionalProperties:
                                  type: number
                            text_offset:
                              type: array
                              items:
                                type: integer
                        finish_reason:
                          type: string
                  usage:
                    required:
                      - prompt_tokens
                      - total_tokens
                      - completion_tokens
                    type: object
                    properties:
                      completion_tokens:
                        type: number
                        format: int32
                      prompt_tokens:
                        type: number
                        format: int32
                      total_tokens:
                        type: number
                        format: int32
              example:
                model: davinci
                object: text_completion
                id: cmpl-4509KAos68kxOqpE2uYGw81j6m7uo
                created: 1637097562
                choices:
                  - index: 0
                    text: The price for bubblegum decreased on thursday.
                    logprobs: 
                    finish_reason: stop
        '400':
          description: Service unavailable
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              schema:
                type: string
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/errorResponse'
              example:
                error:
                  code: string
                  message: string
                  param: string
                  type: string
        '500':
          description: Service unavailable
          headers:
            apim-request-id:
              description: Request ID for troubleshooting purposes
              schema:
                type: string
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/errorResponse'
              example:
                error:
                  code: string
                  message: string
                  param: string
                  type: string
  '/deployments/{deployment-id}/embeddings':
    post:
      summary: Embeddings
      description: Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.
      operationId: embeddings_create
      parameters:
        - name: deployment-id
          in: path
          description: The deployment id of the model which was deployed.
          required: true
          schema:
            enum:
              - embedding
            type: string
            default: embedding
            example: ada-search-index-v1
        - name: api-version
          in: query
          required: true
          schema:
            enum:
              - '2023-05-15'
            type: string
            description: api version
            default: '2023-05-15'
            example: '2023-05-15'
      requestBody:
        content:
          application/json:
            schema:
              required:
                - input
              type: object
              properties:
                input:
                  oneOf:
                    - type: string
                      default: ''
                      nullable: true
                      example: This is a test.
                    - maxItems: 2048
                      minItems: 1
                      type: array
                      items:
                        minLength: 1
                        type: string
                        example: This is a test.
                  description: "Input text to get embeddings for, encoded as a string. To get embeddings for multiple inputs in a single request, pass an array of strings. Each input must not exceed 2048 tokens in length.\nUnless you are embedding code, we suggest replacing newlines (\\n) in your input with a single space, as we have observed inferior results when newlines are present."
                user:
                  type: string
                  description: 'A unique identifier representing your end-user, which can help monitoring and detecting abuse.'
                input_type:
                  type: string
                  description: input type of embedding search to use
                  example: query
                model:
                  type: string
                  description: 'ID of the model to use. You can use the Models_List operation to see all of your available models, or see our Models_Get overview for descriptions of them.'
            example:
              input: { }
              user: string
              input_type: query
              model: string
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                required:
                  - object
                  - model
                  - data
                  - usage
                type: object
                properties:
                  object:
                    type: string
                  model:
                    type: string
                  data:
                    type: array
                    items:
                      required:
                        - index
                        - object
                        - embedding
                      type: object
                      properties:
                        index:
                          type: integer
                        object:
                          type: string
                        embedding:
                          type: array
                          items:
                            type: number
                  usage:
                    required:
                      - prompt_tokens
                      - total_tokens
                    type: object
                    properties:
                      prompt_tokens:
                        type: integer
                      total_tokens:
                        type: integer
              example:
                object: string
                model: string
                data:
                  - index: 0
                    object: string
                    embedding:
                      - 0
                usage:
                  prompt_tokens: 0
                  total_tokens: 0
  '/deployments/{deployment-id}/chat/completions':
    post:
      summary: Chat Completions
      description: Creates a completion for the chat message
      operationId: ChatCompletions_Create
      parameters:
        - name: deployment-id
          in: path
          required: true
          schema:
            enum:
              - gpt-35-turbo
            type: string
            description: Deployment id of the model which was deployed.
            default: gpt-35-turbo
        - name: api-version
          in: query
          required: true
          schema:
            enum:
              - '2023-05-15'
            type: string
            description: api version
            default: '2023-05-15'
            example: '2023-05-15'
      requestBody:
        content:
          application/json:
            schema:
              required:
                - messages
              type: object
              properties:
                messages:
                  minItems: 1
                  type: array
                  items:
                    required:
                      - role
                      - content
                    type: object
                    properties:
                      role:
                        enum:
                          - system
                          - user
                          - assistant
                        type: string
                        description: The role of the author of this message.
                      content:
                        type: string
                        description: The contents of the message
                      name:
                        type: string
                        description: The name of the user in a multi-user chat
                  description: 'The messages to generate chat completions for, in the chat format.'
                temperature:
                  maximum: 2.0
                  minimum: 0.0
                  type: number
                  description: "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both."
                  default: 1
                  nullable: true
                  example: 1
                top_p:
                  maximum: 1.0
                  minimum: 0.0
                  type: number
                  description: "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or `temperature` but not both."
                  default: 1
                  nullable: true
                  example: 1
                n:
                  maximum: 128.0
                  minimum: 1.0
                  type: integer
                  description: How many chat completion choices to generate for each input message.
                  default: 1
                  nullable: true
                  example: 1
                stream:
                  type: boolean
                  description: 'If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.'
                  default: false
                  nullable: true
                stop:
                  oneOf:
                    - type: string
                      nullable: true
                    - maxItems: 4
                      minItems: 1
                      type: array
                      items:
                        type: string
                      description: Array minimum size of 1 and maximum of 4
                  description: Up to 4 sequences where the API will stop generating further tokens.
                  default: 
                max_tokens:
                  type: integer
                  description: 'The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).'
                  default: inf
                presence_penalty:
                  maximum: 2.0
                  minimum: -2.0
                  type: number
                  description: 'Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model''s likelihood to talk about new topics.'
                  default: 0
                frequency_penalty:
                  maximum: 2.0
                  minimum: -2.0
                  type: number
                  description: 'Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model''s likelihood to repeat the same line verbatim.'
                  default: 0
                logit_bias:
                  type: object
                  description: 'Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.'
                  nullable: true
                user:
                  type: string
                  description: 'A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.'
                  example: user-1234
            example:
              model: gpt-35-turbo
              messages:
                - role: user
                  content: Hello!
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                required:
                  - id
                  - object
                  - created
                  - model
                  - choices
                type: object
                properties:
                  id:
                    type: string
                  object:
                    type: string
                  created:
                    type: integer
                    format: unixtime
                  model:
                    type: string
                  choices:
                    type: array
                    items:
                      type: object
                      properties:
                        index:
                          type: integer
                        message:
                          required:
                            - role
                            - content
                          type: object
                          properties:
                            role:
                              enum:
                                - system
                                - user
                                - assistant
                              type: string
                              description: The role of the author of this message.
                            content:
                              type: string
                              description: The contents of the message
                        finish_reason:
                          type: string
                  usage:
                    required:
                      - prompt_tokens
                      - completion_tokens
                      - total_tokens
                    type: object
                    properties:
                      prompt_tokens:
                        type: integer
                      completion_tokens:
                        type: integer
                      total_tokens:
                        type: integer
              example:
                id: chatcmpl-123
                object: chat.completion
                created: 1677652288
                choices:
                  - index: 0
                    message:
                      role: assistant
                      content: "\n\nHello there, how may I assist you today?"
                    finish_reason: stop
                usage:
                  prompt_tokens: 9
                  completion_tokens: 12
                  total_tokens: 21
components:
  schemas:
    errorResponse:
      type: object
      properties:
        error:
          type: object
          properties:
            code:
              type: string
            message:
              type: string
            param:
              type: string
            type:
              type: string
  securitySchemes:
    apiKeyHeader:
      type: apiKey
      name: api-key
      in: header
    apiKeyQuery:
      type: apiKey
      name: subscription-key
      in: query
security:
  - apiKeyHeader: [ ]
  - apiKeyQuery: [ ]