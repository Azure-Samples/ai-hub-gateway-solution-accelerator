<policies>
    <inbound>
        <base />
        <!-- AAD Authorization -->
        <!-- Enabled if entra-validate named value is set to true -->
        <include-fragment fragment-id="aad-auth" />
        
        <!-- Extract model from request payload (avoid generics/encoded chars) -->
        <set-variable name="requestedModel">
            <value><![CDATA[@{
                try
                {
                    var bodyText = context.Request.Body.As<string>(preserveContent: true);
                    if (string.IsNullOrEmpty(bodyText))
                    {
                        return string.Empty;
                    }
                    var body = JObject.Parse(bodyText);
                    return body["model"] != null ? body["model"].ToString() : string.Empty;
                }
                catch
                {
                    return string.Empty;
                }
            }]]></value>
        </set-variable>
        
        <!-- Detect streaming flag from request body (avoid generics/attribute encodings) -->
        <set-variable name="isStream">
            <value><![CDATA[@{
                try
                {
                    var bodyText = context.Request.Body.As<string>(preserveContent: true);
                    if (!string.IsNullOrEmpty(bodyText))
                    {
                        var content = JObject.Parse(bodyText);
                        var token = content["stream"];
                        if (token != null)
                        {
                            return token.ToString().ToLowerInvariant();
                        }
                    }
                }
                catch { }
                return "false";
            }]]></value>
        </set-variable>
        
        <!-- Deleting api-key header so it is not passed to backend endpoints-->
        <set-header name="api-key" exists-action="delete" />
        
        <!-- Setting cache keys -->
        <set-variable name="backendPoolsCacheKey" value="@(&quot;backendPools&quot; + context.Deployment.Region + context.Api.Revision)" />
        <set-variable name="modelMappingsCacheKey" value="@(&quot;modelMappings&quot; + context.Deployment.Region + context.Api.Revision)" />

        <!-- RBAC: Set allowed backends (comma-separated backend-ids, empty means all are allowed) -->
        <set-variable name="allowedBackend" value="" />
        
        <!-- Set default backend pool (optional - set to empty string to return error for unmapped models) -->
        <set-variable name="defaultBackendPool" value="" />

        <!-- Getting backend pools configuration -->
        <cache-lookup-value variable-name="backendPools">
            <key>@{
                var key = context.Variables.ContainsKey("backendPoolsCacheKey")
                    ? (string)context.Variables["backendPoolsCacheKey"]
                    : "ALL-BACKEND-POOLS";
                return key;
            }</key>
        </cache-lookup-value>
        <!-- If we can't find the configuration cached, it will be loaded -->
        <choose>
            <when condition="@(context.Variables.ContainsKey(&quot;backendPools&quot;) == false)">
                <set-variable name="backendPools">
                    <value>@{
                        // Define backend pools with their supported models and routing configurations
                        JArray backendPools = new JArray();
                        
                        // Update the below if condition when using multiple APIM gateway regions/SHGW to get different configurations for each region
                        if(context.Deployment.Region == "West Europe" || true)
                        {
                            // Azure OpenAI Backend Pool
                            var azureOpenAIPool = new JObject()
                            {
                                { "poolName", "azure-openai-backend-pool" },
                                { "poolType", "azure-openai" },
                                { "supportedModels", new JArray("gpt-4o", "gpt-4o-mini", "gpt-35-turbo", "text-embedding-ada-002", "text-embedding-3-small", "text-embedding-3-large") },
                                { "routes", new JArray() }
                            };
                            
                            // Add Azure OpenAI routes
                            ((JArray)azureOpenAIPool["routes"]).Add(new JObject()
                            {
                                { "name", "Azure OpenAI East US" },
                                { "location", "East US" },
                                { "backend-id", "openai-backend-0" },
                                { "priority", 1},
                                { "isThrottling", false }, 
                                { "retryAfter", DateTime.MinValue },
                                { "deploymentMappings", new JObject()
                                    {
                                        { "gpt-4o", "gpt-4o" },
                                        { "gpt-4o-mini", "gpt-4o-mini" },
                                        { "gpt-35-turbo", "chat" },
                                        { "text-embedding-ada-002", "embedding" },
                                        { "text-embedding-3-small", "embedding" },
                                        { "text-embedding-3-large", "embedding" }
                                    }
                                }
                            });
                            
                            ((JArray)azureOpenAIPool["routes"]).Add(new JObject()
                            {
                                { "name", "Azure OpenAI North Central US" },
                                { "location", "North Central US" },
                                { "backend-id", "openai-backend-1" },
                                { "priority", 2},
                                { "isThrottling", false },
                                { "retryAfter", DateTime.MinValue },
                                { "deploymentMappings", new JObject()
                                    {
                                        { "gpt-4o", "gpt-4o" },
                                        { "gpt-4o-mini", "gpt-4o-mini" },
                                        { "gpt-35-turbo", "chat" },
                                        { "text-embedding-ada-002", "embedding" },
                                        { "text-embedding-3-small", "embedding" },
                                        { "text-embedding-3-large", "embedding" }
                                    }
                                }
                            });
                            
                            ((JArray)azureOpenAIPool["routes"]).Add(new JObject()
                            {
                                { "name", "Azure OpenAI East US 2" },
                                { "location", "East US 2" },
                                { "backend-id", "openai-backend-2" },
                                { "priority", 2},
                                { "isThrottling", false },
                                { "retryAfter", DateTime.MinValue },
                                { "deploymentMappings", new JObject()
                                    {
                                        { "gpt-4o", "gpt-4o" },
                                        { "gpt-4o-mini", "gpt-4o-mini" },
                                        { "gpt-35-turbo", "chat" },
                                        { "text-embedding-ada-002", "embedding" },
                                        { "text-embedding-3-small", "embedding" },
                                        { "text-embedding-3-large", "embedding" }
                                    }
                                }
                            });
                            
                            backendPools.Add(azureOpenAIPool);
                            
                            // AI Foundry Llama Backend Pool
                            var aiFoundryLlamaPool = new JObject()
                            {
                                { "poolName", "ai-foundry-llama-pool" },
                                { "poolType", "ai-foundry" },
                                { "supportedModels", new JArray("llama-3.1-405b", "llama-3.1-70b", "llama-3.1-8b", "llama-3.2-90b", "llama-3.2-11b", "llama-3.2-3b", "llama-3.2-1b") },
                                { "routes", new JArray() }
                            };
                            
                            ((JArray)aiFoundryLlamaPool["routes"]).Add(new JObject()
                            {
                                { "name", "AI Foundry Llama East US" },
                                { "location", "East US" },
                                { "backend-id", "ai-foundry-llama-backend-0" },
                                { "priority", 1},
                                { "isThrottling", false },
                                { "retryAfter", DateTime.MinValue },
                                { "deploymentMappings", new JObject()
                                    {
                                        { "llama-3.1-405b", "llama-3-1-405b" },
                                        { "llama-3.1-70b", "llama-3-1-70b" },
                                        { "llama-3.1-8b", "llama-3-1-8b" },
                                        { "llama-3.2-90b", "llama-3-2-90b" },
                                        { "llama-3.2-11b", "llama-3-2-11b" },
                                        { "llama-3.2-3b", "llama-3-2-3b" },
                                        { "llama-3.2-1b", "llama-3-2-1b" }
                                    }
                                }
                            });
                            
                            backendPools.Add(aiFoundryLlamaPool);
                            
                            // AI Foundry Phi Backend Pool
                            var aiFoundryPhiPool = new JObject()
                            {
                                { "poolName", "ai-foundry-phi-pool" },
                                { "poolType", "ai-foundry" },
                                { "supportedModels", new JArray("phi-4", "phi-3.5-mini", "phi-3.5-moe", "phi-3-medium", "phi-3-mini") },
                                { "routes", new JArray() }
                            };
                            
                            ((JArray)aiFoundryPhiPool["routes"]).Add(new JObject()
                            {
                                { "name", "AI Foundry Phi East US" },
                                { "location", "East US" },
                                { "backend-id", "ai-foundry-phi-backend-0" },
                                { "priority", 1},
                                { "isThrottling", false },
                                { "retryAfter", DateTime.MinValue },
                                { "deploymentMappings", new JObject()
                                    {
                                        { "phi-4", "phi-4" },
                                        { "phi-3.5-mini", "phi-3-5-mini" },
                                        { "phi-3.5-moe", "phi-3-5-moe" },
                                        { "phi-3-medium", "phi-3-medium" },
                                        { "phi-3-mini", "phi-3-mini" }
                                    }
                                }
                            });
                            
                            ((JArray)aiFoundryPhiPool["routes"]).Add(new JObject()
                            {
                                { "name", "AI Foundry Phi West US 2" },
                                { "location", "West US 2" },
                                { "backend-id", "ai-foundry-phi-backend-1" },
                                { "priority", 2},
                                { "isThrottling", false },
                                { "retryAfter", DateTime.MinValue },
                                { "deploymentMappings", new JObject()
                                    {
                                        { "phi-4", "phi-4" },
                                        { "phi-3.5-mini", "phi-3-5-mini" },
                                        { "phi-3.5-moe", "phi-3-5-moe" },
                                        { "phi-3-medium", "phi-3-medium" },
                                        { "phi-3-mini", "phi-3-mini" }
                                    }
                                }
                            });
                            
                            backendPools.Add(aiFoundryPhiPool);
                        }
                        else
                        {
                            //No backend pools found for selected region, either return error (default behavior) or set default pools in the else section
                        }
                        
                        return backendPools;   
                    }</value>
                </set-variable>
                <!-- Add backend pools configurations to cache -->
                <cache-store-value duration="86400">
                    <key>@{
                        var key = context.Variables.ContainsKey("backendPoolsCacheKey")
                            ? (string)context.Variables["backendPoolsCacheKey"]
                            : "ALL-BACKEND-POOLS";
                        return key;
                    }</key>
                    <value>@((JArray)context.Variables["backendPools"])</value>
                </cache-store-value>
            </when>
        </choose>

        <!-- Build model to backend pool mapping for faster lookup -->
        <cache-lookup-value variable-name="modelMappings">
            <key>@{
                var key = context.Variables.ContainsKey("modelMappingsCacheKey")
                    ? (string)context.Variables["modelMappingsCacheKey"]
                    : "ALL-MODEL-MAPPINGS";
                return key;
            }</key>
        </cache-lookup-value>
        <choose>
            <when condition="@(context.Variables.ContainsKey(&quot;modelMappings&quot;) == false)">
                <set-variable name="modelMappings">
                    <value>@{
                        JObject modelMappings = new JObject();
                        JArray backendPools = (JArray)context.Variables["backendPools"];
                        
                        foreach (JObject pool in backendPools)
                        {
                            string poolName = pool["poolName"].ToString();
                            JArray supportedModels = (JArray)pool["supportedModels"];
                            
                            foreach (string model in supportedModels)
                            {
                                modelMappings[model] = poolName;
                            }
                        }
                        
                        return modelMappings;
                    }</value>
                </set-variable>
                <!-- Add model mappings to cache -->
                <cache-store-value duration="86400">
                    <key>@{
                        var key = context.Variables.ContainsKey("modelMappingsCacheKey")
                            ? (string)context.Variables["modelMappingsCacheKey"]
                            : "ALL-MODEL-MAPPINGS";
                        return key;
                    }</key>
                    <value>@((JObject)context.Variables["modelMappings"])</value>
                </cache-store-value>
            </when>
        </choose>

        <!-- Determine target backend pool based on requested model -->
        <set-variable name="targetBackendPool">
            <value>@{
                string requestedModel = (string)context.Variables["requestedModel"];
                JObject modelMappings = (JObject)context.Variables["modelMappings"];
                string defaultBackendPool = (string)context.Variables["defaultBackendPool"];
                
                if (string.IsNullOrEmpty(requestedModel))
                {
                    return string.IsNullOrEmpty(defaultBackendPool) ? "ERROR_NO_MODEL" : defaultBackendPool;
                }
                
                if (modelMappings.ContainsKey(requestedModel))
                {
                    return modelMappings[requestedModel].ToString();
                }
                
                return string.IsNullOrEmpty(defaultBackendPool) ? "ERROR_MODEL_NOT_SUPPORTED" : defaultBackendPool;
            }</value>
        </set-variable>

        <!-- Validate target backend pool and return error if needed -->
        <choose>
            <when condition="@(((string)context.Variables[&quot;targetBackendPool&quot;]).StartsWith(&quot;ERROR_&quot;))">
                <choose>
                    <when condition="@(((string)context.Variables[&quot;targetBackendPool&quot;]) == &quot;ERROR_NO_MODEL&quot;)">
                        <return-response>
                            <set-status code="400" reason="Bad Request" />
                            <set-header name="Content-Type" exists-action="override">
                                <value>application/json</value>
                            </set-header>
                            <set-body><![CDATA[@{
                                return new JObject(
                                    new JProperty("error", new JObject(
                                        new JProperty("message", "Model parameter is required in the request payload"),
                                        new JProperty("type", "invalid_request_error"),
                                        new JProperty("code", "missing_model_parameter")
                                    ))
                                ).ToString();
                            }]]></set-body>
                        </return-response>
                    </when>
                    <when condition="@(((string)context.Variables[&quot;targetBackendPool&quot;]) == &quot;ERROR_MODEL_NOT_SUPPORTED&quot;)">
                        <return-response>
                            <set-status code="400" reason="Bad Request" />
                            <set-header name="Content-Type" exists-action="override">
                                <value>application/json</value>
                            </set-header>
                            <set-body><![CDATA[@{
                                string requestedModel = (string)context.Variables["requestedModel"];
                                JObject modelMappings = (JObject)context.Variables["modelMappings"];
                                JArray supportedModels = new JArray();
                                
                                foreach (var kvp in modelMappings)
                                {
                                    supportedModels.Add(kvp.Key);
                                }
                                
                                return new JObject(
                                    new JProperty("error", new JObject(
                                        new JProperty("message", $"Model '{requestedModel}' is not supported. Supported models: {string.Join(", ", supportedModels.Select(m => m.ToString()))}"),
                                        new JProperty("type", "invalid_request_error"),
                                        new JProperty("code", "unsupported_model"),
                                        new JProperty("param", "model"),
                                        new JProperty("supported_models", supportedModels)
                                    ))
                                ).ToString();
                            }]]></set-body>
                        </return-response>
                    </when>
                </choose>
            </when>
        </choose>

        <!-- Get routes for the selected backend pool -->
        <set-variable name="selectedPoolRoutes">
            <value>@{
                string targetBackendPool = (string)context.Variables["targetBackendPool"];
                JArray backendPools = (JArray)context.Variables["backendPools"];
                
                foreach (JObject pool in backendPools)
                {
                    if (pool["poolName"].ToString() == targetBackendPool)
                    {
                        return (JArray)pool["routes"];
                    }
                }
                
                return new JArray();
            }</value>
        </set-variable>

        <!-- Validate routes and filter for backend routing -->
        <include-fragment fragment-id="validate-routes" />
        
        <!-- Set authentication based on backend pool type -->
        <choose>
            <when condition="@{
                string targetBackendPool = (string)context.Variables[&quot;targetBackendPool&quot;];
                JArray backendPools = (JArray)context.Variables[&quot;backendPools&quot;];
                
                foreach (JObject pool in backendPools)
                {
                    if (pool[&quot;poolName&quot;].ToString() == targetBackendPool)
                    {
                        return pool[&quot;poolType&quot;].ToString() == &quot;azure-openai&quot;;
                    }
                }
                return false;
            }">
                <!-- Azure OpenAI Backend Managed Identity -->
                <authentication-managed-identity resource="https://cognitiveservices.azure.com" output-token-variable-name="msi-access-token" client-id="{{uami-client-id}}" ignore-error="false" />
                <set-header name="Authorization" exists-action="override">
                    <value>@(&quot;Bearer &quot; + (string)context.Variables[&quot;msi-access-token&quot;])</value>
                </set-header>
            </when>
            <otherwise>
                <!-- AI Foundry Backend Managed Identity -->
                <authentication-managed-identity resource="https://ml.azure.com" output-token-variable-name="msi-access-token" client-id="{{uami-client-id}}" ignore-error="false" />
                <set-header name="Authorization" exists-action="override">
                    <value>@(&quot;Bearer &quot; + (string)context.Variables[&quot;msi-access-token&quot;])</value>
                </set-header>
            </otherwise>
        </choose>

        <!-- Setting global TPM limit to collect usage for streaming requests -->
        <azure-openai-token-limit counter-key="UniversalLLMAPI" tokens-per-minute="50000000" estimate-prompt-tokens="false" tokens-consumed-variable-name="TotalConsumedTokens" remaining-tokens-variable-name="TotalRemainingTokens" />
        
        <!-- Handling usage for streaming requests -->
        <include-fragment fragment-id="openai-usage-streaming" />
    </inbound>
    <backend>
        <include-fragment fragment-id="backend-routing" />
    </backend>
    <outbound>
        <base />

        <!-- Handling usage for non-streaming requests -->
        <include-fragment fragment-id="openai-usage" />
    </outbound>
    <on-error>
        <base />
        <!-- This is used to push custom metrics related to 429 throttling errors -->
        <!-- It is designed to permit setting up Azure Monitor Alerts notifying the team of potential service degradation -->
        <set-variable name="service-name" value="Universal LLM API" />
        <set-variable name="target-deployment" value="@((string)context.Variables[&quot;requestedModel&quot;])" />
        <include-fragment fragment-id="throttling-events" />
    </on-error>
</policies>