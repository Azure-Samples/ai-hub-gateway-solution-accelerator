{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè∞ Citadel Governance Hub - Testing Center\n",
    "\n",
    "## Test your governance hub managed models, enabled through Azure API Management!\n",
    "\n",
    "Use this Jupyter notebook with Python code snippets to verify proper functionality of your governance hub managed models when accessed through AI Gateway features in Azure API Management (APIM) part of AI Citadel Governance Hub.\n",
    "\n",
    "> **Note:** This notebook assumes you have already set up your Citadel Governance Hub and have models deployed and managed through it. If you haven't done so, please refer to the [Citadel Governance Hub Deployment Guide](../guides/full-deployment-guide.md) or [Citadel Governance Hub Quick Deployment Guide](../guides/quick-deployment-guide.md) before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### ‚öôÔ∏è Initialize client tool for your APIM service\n",
    "\n",
    "üëâ An existing Azure AI Foundry API is expected to be already configured on APIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, requests\n",
    "sys.path.insert(1, '../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "from apimtools import APIMClientTool\n",
    "\n",
    "inference_api_version = \"2024-05-01-preview\"\n",
    "\n",
    "targetInferenceApi = \"models\" # use 'models' for universal LLM API, or 'openai' for Azure OpenAI\n",
    "\n",
    "try:\n",
    "    apimClientTool = APIMClientTool(\n",
    "        \"rg-ai-hub-citadel-dev-01\" ## specify the resource group name where the API Management resource is located, or optionally add another parameter with the apim_resource_name\n",
    "    )\n",
    "    apimClientTool.initialize()\n",
    "    apimClientTool.discover_api(targetInferenceApi) # use 'models' for inference API 'openai' for Azure OpenAI\n",
    "\n",
    "    apim_resource_gateway_url = str(apimClientTool.apim_resource_gateway_url)\n",
    "    azure_endpoint = str(apimClientTool.azure_endpoint)\n",
    "\n",
    "    api_key = apimClientTool.apim_subscriptions[1].get(\"key\") # Ensure that you have created a subscription in APIM\n",
    "\n",
    "    # Get supported models from the policy fragment\n",
    "    supported_models = apimClientTool.get_policy_fragment_supported_models(\"set-backend-pools\")\n",
    "    utils.print_info(f\"Supported models in APIM policy fragment 'set-backend-pools': {supported_models}\")\n",
    "    # model_name = supported_models[2]  # pick the third model from the supported models in the policy fragment\n",
    "\n",
    "    if targetInferenceApi == \"openai\":\n",
    "        chat_completions_url = f\"{azure_endpoint}openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "    else:  # models\n",
    "        chat_completions_url = f\"{azure_endpoint}models/chat/completions?api-version={inference_api_version}\"\n",
    "    utils.print_info(f\"Chat Completion Endpoint: {chat_completions_url}\")\n",
    "\n",
    "    utils.print_ok(f\"Testing tool initialized successfully!\")\n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error initializing APIM Client Tool: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = supported_models[2] # pick the model from the supported models in the policy fragment\n",
    "utils.print_info(f\"Using model: {model_name}\")\n",
    "\n",
    "api_key = apimClientTool.apim_subscriptions[6].get(\"key\") # Ensure that you have created a subscription in APIM\n",
    "\n",
    "messages={\"model\": model_name, \"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "\n",
    "\n",
    "response = requests.post(chat_completions_url, headers = {'api-key':api_key}, json = messages)\n",
    "utils.print_response_code(response)\n",
    "utils.print_info(f\"headers {response.headers}\")\n",
    "utils.print_info(f\"x-ms-region: {response.headers.get(\"x-ms-region\")}\") # this header is useful to determine the region of the backend that served the request\n",
    "if (response.status_code == 200):\n",
    "    data = json.loads(response.text)\n",
    "    print(\"üí¨ \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "else:\n",
    "    utils.print_error(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, requests\n",
    "sys.path.insert(1, '../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "from apimtools import APIMClientTool\n",
    "\n",
    "inference_api_version = \"2024-05-01-preview\"\n",
    "\n",
    "targetInferenceApi = \"models\" # use 'models' for universal LLM API, or 'openai' for Azure OpenAI\n",
    "\n",
    "try:\n",
    "    apimClientTool = APIMClientTool(\n",
    "        \"rg-ai-hub-citadel-dev-01\" ## specify the resource group name where the API Management resource is located, or optionally add another parameter with the apim_resource_name\n",
    "    )\n",
    "    apimClientTool.initialize()\n",
    "    apimClientTool.discover_api(targetInferenceApi) # use 'models' for inference API 'openai' for Azure OpenAI\n",
    "\n",
    "    apim_resource_gateway_url = str(apimClientTool.apim_resource_gateway_url)\n",
    "    azure_endpoint = str(apimClientTool.azure_endpoint)\n",
    "\n",
    "    api_key = apimClientTool.apim_subscriptions[1].get(\"key\") # Ensure that you have created a subscription in APIM\n",
    "\n",
    "    # Get supported models from the policy fragment\n",
    "    supported_models = apimClientTool.get_policy_fragment_supported_models(\"set-backend-pools\")\n",
    "    utils.print_info(f\"Supported models in APIM policy fragment 'set-backend-pools': {supported_models}\")\n",
    "    model_name = supported_models[2]  # pick the third model from the supported models in the policy fragment\n",
    "\n",
    "    if targetInferenceApi == \"openai\":\n",
    "        chat_completions_url = f\"{azure_endpoint}openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "    else:  # models\n",
    "        chat_completions_url = f\"{azure_endpoint}models/chat/completions?api-version={inference_api_version}\"\n",
    "    utils.print_info(f\"Chat Completion Endpoint: {chat_completions_url}\")\n",
    "\n",
    "    utils.print_ok(f\"Testing tool initialized successfully!\")\n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error initializing APIM Client Tool: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Send multiple requests within one minute to surpass the established token rate limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "# Run for 1 minute (60 seconds)\n",
    "api_runs = []\n",
    "start_time = time.time()\n",
    "run_count = 0\n",
    "messages={\"messages\":[\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "\n",
    "model_name = supported_models[0] # pick the model from the supported models in the policy fragment\n",
    "\n",
    "api_key = apimClientTool.apim_subscriptions[6].get(\"key\") # Ensure that you have created a subscription in APIM\n",
    "chat_completions_url = f\"{azure_endpoint}/openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "\n",
    "print(f\"üïê Starting API calls for 1 minute...\")\n",
    "print(f\"Start time: {time.strftime('%H:%M:%S', time.localtime(start_time))}\")\n",
    "\n",
    "while (time.time() - start_time) < 60:  # Run for 60 seconds\n",
    "    run_count += 1    \n",
    "    call_start_time = time.time()\n",
    "    response = requests.post(chat_completions_url, headers = {'api-key':api_key}, json = messages)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if (response.status_code == 200):\n",
    "        print(f\"‚ñ∂Ô∏è Run: {run_count} | {elapsed_time:.1f}s | status: {response.status_code} ‚úÖ\")\n",
    "        data = json.loads(response.text)\n",
    "        total_tokens = data.get(\"usage\").get(\"total_tokens\")\n",
    "        print(f\"    consumed tokens: {response.headers.get('consumed-tokens')}, remaining tokens: {response.headers.get('remaining-tokens')}\")\n",
    "    else:\n",
    "        print(f\"‚ñ∂Ô∏è Run: {run_count} | {elapsed_time:.1f}s | status: {response.status_code} ‚õî\")        \n",
    "        print(f\"    error: {response.text}\")\n",
    "        total_tokens = 0\n",
    "    \n",
    "    api_runs.append((call_start_time, total_tokens, response.status_code))\n",
    "    time.sleep(0.1) # Small delay to prevent overwhelming the API\n",
    "\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "print(f\"\\nüèÅ Completed {run_count} API calls in {total_duration:.1f} seconds\")\n",
    "print(f\"Average rate: {run_count / total_duration:.2f} calls/second\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Token Rate limiting results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "if 'api_runs' in locals() and api_runs:\n",
    "    calls = [(t - api_runs[0][0], tokens or 0, status) for t, tokens, status in api_runs]\n",
    "    capacity = 1000\n",
    "    refill = capacity / 60\n",
    "    bucket = capacity\n",
    "    last_time = 0.0\n",
    "    times, usage, status_codes, levels = [], [], [], []\n",
    "\n",
    "    for call_time, tokens, status in calls:\n",
    "        bucket = min(capacity, bucket + (call_time - last_time) * refill)\n",
    "        levels.append(bucket)\n",
    "        times.append(call_time)\n",
    "        usage.append(tokens)\n",
    "        status_codes.append(status)\n",
    "        bucket = max(0, bucket - tokens)\n",
    "        last_time = call_time\n",
    "\n",
    "    colors = ['tab:green' if code == 200 else 'tab:red' if code == 429 else 'tab:orange' for code in status_codes]\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.bar(times, usage, color=colors, width=0.35, alpha=0.7)\n",
    "    ax2.plot(times, levels, color='purple', linewidth=2)\n",
    "    ax2.axhline(capacity, color='purple', linestyle='--', alpha=0.6)\n",
    "\n",
    "    throttled_times = [t for t, code in zip(times, status_codes) if code == 429]\n",
    "    throttled_usage = [u for u, code in zip(usage, status_codes) if code == 429]\n",
    "    if throttled_times:\n",
    "        max_usage = max(usage) if usage else 0\n",
    "        throttled_marker_heights = [u + max_usage * 0.01 for u in throttled_usage]\n",
    "        ax1.scatter(throttled_times, throttled_marker_heights, marker='o', s=20, color='darkred', edgecolors='white', linewidth=0.4, zorder=6)\n",
    "\n",
    "    ax1.set_xlabel('Seconds')\n",
    "    ax1.set_ylabel('Tokens per call')\n",
    "    ax2.set_ylabel('Tokens in bucket')\n",
    "    ax1.set_title('Token bucket behaviour over 60 seconds')\n",
    "\n",
    "    legend_items = [\n",
    "        Patch(facecolor='tab:green', alpha=0.7, label='Success (200)'),\n",
    "        Line2D([0], [0], color='purple', linewidth=2, label='Bucket level'),\n",
    "        Line2D([0], [0], color='purple', linestyle='--', label='Capacity'),\n",
    "        Line2D([0], [0], marker='o', color='darkred', markersize=8, linestyle='None',\n",
    "                markerfacecolor='darkred', markeredgecolor='white', label='Throttled (429)')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_items, loc='upper right', bbox_to_anchor=(0.98, 0.85), framealpha=0.9)\n",
    "\n",
    "    success = sum(code == 200 for code in status_codes)\n",
    "    throttled = sum(code == 429 for code in status_codes)\n",
    "    print(f\"Calls: {len(status_codes)} | Success: {success} | 429s: {throttled}\")\n",
    "else:\n",
    "    print('Run the 60-second API test first to capture api_runs data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test with streaming using the Azure OpenAI Python SDK\n",
    "With a streaming API call, the response is sent back incrementally in chunks via an [event stream](https://developer.mozilla.org/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format). In Python, you can iterate over these events with a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "messages=[\n",
    "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=chat_completions_url,\n",
    "    api_key=api_key,\n",
    "    api_version=inference_api_version\n",
    ")\n",
    "\n",
    "model_name = supported_models[2] # pick the model from the supported models in the policy fragment\n",
    "utils.print_info(f\"Using model: {model_name}\")\n",
    "\n",
    "response = client.chat.completions.with_raw_response.create(model=model_name, messages=messages, stream=True)\n",
    "\n",
    "print(\"headers \", response.headers)\n",
    "print(\"x-ms-region: \", response.headers.get(\"x-ms-region\")) # this header is useful to determine the region of the backend that served the request\n",
    "print(\"x-ms-stream: \", response.headers.get(\"x-ms-stream\")) # this header is useful to determine if the response is streamed\n",
    "\n",
    "completion = response.parse() \n",
    "\n",
    "# create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "# iterate through the stream of events\n",
    "for chunk in completion:\n",
    "    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    if chunk.choices:\n",
    "        chunk_message = chunk.choices[0].delta.content  # extract the message\n",
    "        collected_messages.append(chunk_message)  # save the message\n",
    "        print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "# clean None in collected_messages\n",
    "collected_messages = [m for m in collected_messages if m is not None]\n",
    "full_reply_content = ''.join(collected_messages)\n",
    "print(f\"Full conversation received: {full_reply_content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Execute multiple runs for each subscription using the Azure OpenAI Python SDK\n",
    "\n",
    "We will send requests for each subscription. Adjust the `sleep_time_ms` and the number of `runs` to your test scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "runs = 10\n",
    "sleep_time_ms = 100\n",
    "\n",
    "model_name = supported_models[2] # pick the model from the supported models in the policy fragment\n",
    "utils.print_info(f\"Using model: {model_name}\")\n",
    "\n",
    "clients = [\n",
    "    AzureOpenAI(\n",
    "        azure_endpoint = chat_completions_url,\n",
    "        api_key = apimClientTool.apim_subscriptions[6].get(\"key\"),\n",
    "        api_version = inference_api_version\n",
    "    ),\n",
    "    AzureOpenAI(\n",
    "        azure_endpoint = chat_completions_url,\n",
    "        api_key = apimClientTool.apim_subscriptions[6].get(\"key\"),\n",
    "        api_version = inference_api_version\n",
    "    ),\n",
    "    AzureOpenAI(\n",
    "        azure_endpoint = chat_completions_url,\n",
    "        api_key = apimClientTool.apim_subscriptions[6].get(\"key\"),\n",
    "        api_version = inference_api_version\n",
    "    )\n",
    "]\n",
    "\n",
    "for i in range(runs):\n",
    "    print(f\"‚ñ∂Ô∏è Run {i+1}/{runs}:\")\n",
    "\n",
    "    for j in range(0, 3):\n",
    "        response = clients[j].chat.completions.create(\n",
    "            model = model_name,\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "            ],\n",
    "            extra_headers = {\"x-user-id\": \"alex\"}\n",
    "        )\n",
    "        print(f\"üí¨ Subscription {j+1}: {response.choices[0].message.content}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    time.sleep(sleep_time_ms/1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
